import numpy as np
import pandas as pd
import json
import dataclasses
from dataclasses import dataclass, fields, _MISSING_TYPE
from heapq import heappush, heappop, heappushpop
from collections import OrderedDict
import math
import matplotlib.pyplot as plt
import spacy
from spacy import displacy
from entropy_calculation import uncertainty
from state_utils import get_spacy_info, get_llm_info, align_llm_spacy_output, get_model_state, StateOverview
from sentence_transformers import SentenceTransformer
from numpy.linalg import norm
from scipy.spatial.distance import cityblock, minkowski, jaccard
import yake

KEYWORD_TAG = set(["VERB", "AUX", "NOUN", "ADJ", "ADV"])

class SimilarityScore():
    """
    Copmute different similarity scores from passage, sentence and token(state) levels
    """
    def __init__(self, response:str, answer:str, final_states:list, nlp, model, weights:dict):
        self.final_states = final_states    # response states
        self.response = response            # reaponse(answer) generated by LLM
        self.answer = answer                # true label answer
        self.nlp = nlp                      # spacy pipeline
        self.model = model                  # model for sentence embedding
        self.weights = weights              # weights for different scores
        self.response_doc, self.answer_doc = self.nlp(self.response), self.nlp(self.answer)
        self.passage_score = self.compute_passage_score()   # passage-level score
        self.sentence_score = self.compute_sentence_score() # sentence-level score
        self.token_score = self.compute_token_score()       # token-level score
        self.keywords = self.keyword_extraction()           # extracted keywords
        self.keyword_score = self.copmute_keyword_score(self.keywords)  # keywords score
        # alternative token cosine similarity score
        # alternative token pos_tag alignmnet score
        self.alter_token_sim_score, self.alter_token_pos_score = self.alter_token_alignmnet_score()
        self.state_score = self.compute_state_score(self.passage_score, self.sentence_score,self.token_score, 
                                                    self.keyword_score, self.alter_token_sim_score,
                                                    self.alter_token_pos_score, self.weights)

    def _my_normalization(self, values, lower_bound, upper_bound):
        """
        [0,1] normalization
        """ 
        # return (x-np.min(x))/(np.max(x)-np.min(x))
        return [lower_bound + (x - np.min(values)) * (upper_bound - lower_bound) / (np.max(values) - np.min(values)) for x in values]



    def _compute_embedding_score(self, target_1:str, target_2:str):
        """
        Compute cosine, euclidean, dot, etc similarity/distance based on embedding vectors
        """
        vec_1, vec_2 = self.model.encode([target_1, target_2])   # get vectors
        cosine = np.dot(vec_1, vec_2)/(norm(vec_1)*norm(vec_2))      # compute cosine similarity
        euclidean = math.dist(vec_1, vec_2)             # compute Euclidean distance
        dot_product = np.dot(vec_1, vec_2)              # compute dot product
        manhattan = cityblock(vec_1, vec_2)             #calculate Manhattan distance between vectors
        minkowski_distance = minkowski(vec_1, vec_2)    # Minkowski distance
        jaccard_sim = jaccard(vec_1, vec_2)             # Jaccard similarity

        result = {"cosine": cosine,
                  "euclidean": euclidean,
                  "dot_product": dot_product,
                  "manhattan": manhattan,
                  "minkowski": minkowski_distance,
                  "jaccard": jaccard_sim}
        return result

    def compute_passage_score(self):
        """
        Compute passage-level similarity scores 
        """
        result = self._compute_embedding_score(self.response_doc.text, self.answer_doc.text)
        return result

    def compute_sentence_score(self):
        """
        Compute sentence-level similarity scores 
        """
        result = list() # list(dict) one dict for each sentence
        start_index = 0
        # loop each sentence
        for end_index in self.final_states.end_state_index:
            sentence_doc = self.response_doc[start_index:end_index]
            result.append(self._compute_embedding_score(sentence_doc.text, self.answer_doc.text))
            start_index = end_index
        return result
    
    def compute_token_score(self):
        """
        Compute token-level similarity scores 
        """
        result = list() # list(dict) one dict for each token
        for state in self.final_states.states:
            result.append(self._compute_embedding_score(self.response_doc[state.idx].text, self.answer_doc.text))
        return result
    
    def keyword_extraction(self):
        """
        Extract keywords from true answer
        """
        # answer lemma for simplified comparison
        answer_lemma = repr(' '.join([token.lemma_ for token in self.answer_doc]))
        # num of state with pos_tag in NOUN and VERB
        num_NOUN_VERB = sum([state.prop_tag.pos_tag in KEYWORD_TAG for state in self.final_states.states])
        language = "en"
        max_ngram_size = 1                  # n-gram
        deduplication_threshold = 0.9       # 0.1 to avoid the repetition, 0.9 to allow repetition
        numOfKeywords = num_NOUN_VERB       # num of NOUN and VERB
        custom_kw_extractor = yake.KeywordExtractor(lan=language,      
                                             n=max_ngram_size,  
                                             dedupLim=deduplication_threshold, 
                                             top=numOfKeywords, 
                                             features=None)
        keywords = custom_kw_extractor.extract_keywords(answer_lemma)      
        keys = list(dict(keywords).keys())
        # The lower the score, the more relevant the keyword is
        # Reverse to the higher the sore, the more relevant the keyword is
        scores = np.array(list(dict(keywords).values()))
        scores = self._my_normalization(1-scores, 0.5, 1)   # normalization
        return dict(zip(keys, scores))
    
    def copmute_keyword_score(self, keywords:dict):
        """
        Compute keywrods score, if a state belongs to the extracted keywords
        """
        result = list()     # keyword socre for each state
        words = list(keywords.keys())
        for state in self.final_states.states:
            # if state has important pos_tag and belongs to keywords
            state_lemma = state.spacy_info["lemma_"]
            if (state.prop_tag.pos_tag in KEYWORD_TAG) and (state_lemma in words):
                result.append(keywords[state_lemma])
            # if state does not have important pos_tag but belongs to keywords
            elif state_lemma in words:
                result.append((keywords[state_lemma])*0.5)    # reduced score by 50%
            else:
                result.append(0)
        return result
    

    def alter_token_alignmnet_score(self):
        """
        Compute alternative token cosine similarity and POS alignmnet socre
        """
        alter_sim_result = list() # list(dict) one dict for each token
        alter_pos_result = list()
        for state in self.final_states.states:
            alter_token_pos = list()        # pos tag of alternative tokens 
            alter_token_sim = list()        # cosine similarity between state token and state alternative tokens
            alter_tokens = [alter_token for alter_token in state.llm_info[0]["top_k_token"][1:]]
            
            # loop alternative tokens for all states
            for alter_token in alter_tokens:
                token = self.nlp(alter_token)
                if len(token) > 1:      # avoid " ", "\n\n", etc.
                    token = token[1]
                else:
                    token = token[0]
                vec_1, vec_2 = self.model.encode([self.response_doc[state.idx].text, token.text])
                alter_token_sim.append(np.dot(vec_1, vec_2)/(norm(vec_1)*norm(vec_2)))
                alter_token_pos.append(token.pos_)
            # average alter token cosine similarity
            alter_sim_result.append(np.mean(alter_token_sim))   
            # percentage of alter tokens which have same pos_tag with state token
            alter_pos_result.append(np.sum(np.array(alter_token_pos) == state.prop_tag.pos_tag)/len(alter_token_pos))
        return self._my_normalization(alter_sim_result, 0, 1), alter_pos_result
        
    def compute_state_score(self, passage_score:dict, sentence_score:list, token_score:list, keyword_score:list,
                            alter_token_sim_score:list, alter_token_pos_score:list, weights:dict):
        """
        Compute final score for each state
        """
        result = list()
        for state in self.final_states.states:
            state_score = 0 # TBD