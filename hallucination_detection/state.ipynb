{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State abstraction for GPT3 \n",
    "\n",
    "Modified from  <code> state.py </code>\n",
    "\n",
    "This script is used to mount semantics info for each toekn/word in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "        #                     _ooOoo_  \n",
    "        #                    o8888888o  \n",
    "        #                    88\" . \"88  \n",
    "        #                    (| -_- |)  \n",
    "        #                     O\\ = /O  \n",
    "        #                 ____/`---'\\____  \n",
    "        #               .   ' \\\\| |// `.  \n",
    "        #                / \\\\||| : |||// \\  \n",
    "        #              / _||||| -:- |||||- \\  \n",
    "        #                | | \\\\\\ - /// | |  \n",
    "        #              | \\_| ''\\---/'' | |  \n",
    "        #               \\ .-\\__ `-` ___/-. /  \n",
    "        #            ___`. .' /--.--\\ `. . __  \n",
    "        #         .\"\" '< `.___\\_<|>_/___.' >'\"\".  \n",
    "        #        | | : `- \\`.;`\\ _ /`;.`/ - ` : | |  \n",
    "        #          \\ \\ `-. \\_ __\\ /__ _/ .-` / /  \n",
    "        #  ======`-.____`-.___\\_____/___.-`____.-'======  \n",
    "        #                     `=---='  \n",
    "  \n",
    "        #  .............................................  \n",
    "        #           佛祖保佑             永无BUG \n",
    "        #   佛曰:  \n",
    "        #           写字楼里写字间，写字间里程序员；  \n",
    "        #           程序人员写程序，又拿程序换酒钱。  \n",
    "        #           酒醒只在网上坐，酒醉还来网下眠；  \n",
    "        #           酒醉酒醒日复日，网上网下年复年。  \n",
    "        #           但愿老死电脑间，不愿鞠躬老板前；  \n",
    "        #           奔驰宝马贵者趣，公交自行程序员。  \n",
    "        #           别人笑我忒疯癫，我笑自己命太贱；  \n",
    "        #           不见满街漂亮妹，哪个归得程序员？"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Universal POS tags**\n",
    "\n",
    "| Open class word | Close class word | Other |\n",
    "| --- | --- | --- |\n",
    "| ADJ, ADV, INTJ, NOUN, PRON, VERB | ADP, AUX, CCONG, DET, NUM, PART, PRON, SCONJ | PUNCT, SYM, X |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import dataclasses\n",
    "from dataclasses import dataclass, fields, _MISSING_TYPE\n",
    "from heapq import heappush, heappop, heappushpop\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from entropy_calculation import uncertainty\n",
    "\n",
    "\n",
    "\n",
    "SPACY_INFO_DICT = {\"text\":[], \"lemma_\":[], \"pos_\":[], \n",
    "                  \"tag_\":[], \"dep_\":[], \"shape_\":[], \"is_alpha\":[],\n",
    "                 \"is_stop\":[], \"i\":[]}\n",
    "\n",
    "# Universal POS tags: https://universaldependencies.org/u/pos/\n",
    "POS_NOUN = set([\"NOUN\", \"PRON\", \"PROPN\", \"NN\", \"NNP\", \"NNPS\", \"NNS\", \"NE\", \"NNE\"])\n",
    "POS_VERB = set([\"VERB\", \"AUX\"])\n",
    "NOUN_CHILDREN = set([\"NOUN\", \"PRON\", \"PROPN\", \"ADJ\"])\n",
    "VERB_CHILDREN = set([\"NOUN\", \"PRON\", \"PROPN\", \"ADV\"])\n",
    "\n",
    "# @dataclass(unsafe_hash=True)\n",
    "class StateStatics:\n",
    "    \"\"\"\n",
    "    Class to collect statistics at state level\n",
    "    1. moving statistics \n",
    "    2. weighted statistics\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.moving_doc_median = dict()\n",
    "        self.moving_rmse = dict()\n",
    "        self.moving_mean = dict()\n",
    "        self.weighted_moving_doc_median = dict()   # weighted moving statistics\n",
    "        self.weighted_moving_rmse = dict()\n",
    "        self.weighted_moving_mean = dict()\n",
    "        self.alter_token = dict()       # alternative token, word similarity, pos_tag: {\"alter_token_1\": [similarity, pos_tag], ...}\n",
    "        self.weighted_prob = 0.0        # weighted prob w.r.t syntactic children\n",
    "        self.weighted_entropy = 0.0     # weighted entropy w.r.t syntactic children\n",
    "\n",
    "@dataclass(eq=False)\n",
    "class PropTag:\n",
    "    \"\"\"\n",
    "    Class to represent the property tag of each token\n",
    "    Convert POS tags to equivalent property tags\n",
    "    \"\"\"\n",
    "    # pos_tags: list = None          # Can there exist multiple pos_tages for one token?\n",
    "    pos_tag: str            # Coarse-grained part-of-speech\n",
    "    propstr: str = None     # map an equivalent property to the same hashable value\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.propstr = self._hash_map_func(self.pos_tag)\n",
    "    \n",
    "    # equivalent property tag mapping\n",
    "    def _hash_map_func(self, prop):\n",
    "        if prop in POS_NOUN:\n",
    "            return \"NOUN\"\n",
    "        elif prop in POS_VERB:\n",
    "            return \"VERB\"\n",
    "        else:\n",
    "            return prop\n",
    "\n",
    "@dataclass(unsafe_hash=True)\n",
    "class State:\n",
    "    \"\"\" \n",
    "    Construct the basic state following the aligned outputs from llm and spacy\n",
    "    Additional attributes will be added in later processes \n",
    "    \"\"\"\n",
    "    idx: int            # token idx w.r.t entire passage\n",
    "    token: str          # text based on spacy\n",
    "    entropy: float      # Shannon entropy \n",
    "    probability: float  # highest prob\n",
    "    llm_info: list      # one word in spact may represent serval tokens in llm\n",
    "    spacy_info: dict    # nlp_result\n",
    "    summary: dict       # all token info for backup\n",
    "    weights: float= None            # weights assigned for syntactic children\n",
    "    prop_tag: PropTag = None\n",
    "    statistics: StateStatics = None\n",
    "    children: dict = None           # syntactic children\n",
    "        \n",
    "class StateOverview:\n",
    "    \"\"\"\n",
    "    Copmute some statistical info w.r.t both state and passage levels:\n",
    "    1. split states by sentences\n",
    "    2. compute moving statisics (sliding window)\n",
    "    3. compute weighted statisics based on syntactic children\n",
    "    \"\"\"\n",
    "    def __init__(self, states:list, window_len:int, target_tags:set, children_weights:float, mode:str, nlp_model):\n",
    "        self.states = states            # constructed state\n",
    "        self.window_len = window_len    # sliding window for interval info\n",
    "        self.target_tags = target_tags    # copmute wighted statistics for tokens with target tags \n",
    "        self.children_weights = children_weights  # weights for dependent child states\n",
    "        self.sentence_state, self.end_state_index = self.split_states_by_sentence(states)\n",
    "        self.moving_mdoe = mode         # passage level or sentence level\n",
    "        self.nlp_model = nlp_model      # spacy model to process alternative tokens\n",
    "        # self.prop_link_map = self.generate_property_link(states)\n",
    "        \n",
    "        self.compute_weighted_statistics()\n",
    "        self.compute_moving_statistics()\n",
    "        self.compute_alter_token_statistics()\n",
    "\n",
    "    def compute_alter_token_statistics(self):\n",
    "        \"\"\"\n",
    "        Get alternative tokens for each state, get word similarity score, pos_tag, probability\n",
    "        \"\"\"\n",
    "        for state in self.states:\n",
    "            alter_tokens = state.llm_info[0][\"top_k_token\"][1:]    # if state contains multiple llm toknes, take the first one\n",
    "            \n",
    "            alter_token_score = list()          # get semantic similarity score (cosine over vectors)\n",
    "            for token in alter_tokens:      \n",
    "                if not self.nlp_model(token).has_vector:    # if empty vectors enountered\n",
    "                    alter_token_score.append(0)\n",
    "                else:\n",
    "                    alter_token_score.append(self.nlp_model(state.token).similarity(self.nlp_model(token)))\n",
    "            # alter_token_score = [self.nlp_model(state.token).similarity(self.nlp_model(token)) for token in alter_tokens]\n",
    "\n",
    "            alter_toekn_pos_tag = [self.nlp_model(token)[0].pos_ for token in alter_tokens] # if multiple token available, take the first one\n",
    "            alter_token_prob = state.llm_info[0][\"top_k_prob\"]     # alter token probabilities\n",
    "            for i, token in enumerate(alter_tokens):\n",
    "                state.statistics.alter_token[token] = {\"similarity\": alter_token_score[i],\n",
    "                                                        \"pos_tag\": alter_toekn_pos_tag[i],\n",
    "                                                        \"probability\": alter_token_prob[i]}\n",
    "\n",
    "\n",
    "    def split_states_by_sentence(self, states):\n",
    "        \"\"\"\n",
    "        Splite states by sentences\n",
    "        Return: segmented states(by sentence), index of end states\n",
    "        \"\"\"\n",
    "        result = list()     # states split by sentences [[sentence 1], [sentence 2], ...]\n",
    "        end_state_index = list()    # dix of states which are the ends of sentences\n",
    "        new_start = 0\n",
    "        for i, state in enumerate(states):\n",
    "            if state.spacy_info[\"new_sentence\"]:\n",
    "                try: result.append(states[new_start:i+1])  # if the passage not end with \".\"\n",
    "                except: result.append(states[new_start:i])\n",
    "                end_state_index.append(i)\n",
    "                new_start = i+1\n",
    "        return result, end_state_index\n",
    "    \n",
    "    def compute_weighted_statistics(self):\n",
    "        \"\"\"\n",
    "        Assign weighted probability, entropy to state with target POS tags\n",
    "        \"\"\"\n",
    "        for state in self.states:\n",
    "            # if state with target tag also has children\n",
    "            if (state.prop_tag.propstr in self.target_tags) and len(state.children)>0:\n",
    "                children_states = [self.states[i] for i in list(state.children.values())]\n",
    "\n",
    "                # wieghted prob = (1-weight)*state_prob + weight*avg(children_probs)\n",
    "                state.statistics.weighted_prob = (1.0-self.children_weights)*state.probability + \\\n",
    "                        self.children_weights*np.average([child.probability for child in children_states])\n",
    "                \n",
    "                # wieghted entropy: same as above\n",
    "                state.statistics.weighted_entropy = (1.0-self.children_weights)*state.entropy + \\\n",
    "                        self.children_weights*np.average([child.entropy for child in children_states])\n",
    "            else:\n",
    "                state.statistics.weighted_entropy = state.entropy\n",
    "                state.statistics.weighted_prob = state.probability\n",
    "\n",
    "\n",
    "    def _add_num_for_stat(self, num, list_of_stat):\n",
    "        for i in list_of_stat:\n",
    "            i.add_num(num)\n",
    "\n",
    "    def compute_moving_statistics(self):\n",
    "        \"\"\"\n",
    "        Compute interval/moving statistics for state probability and entropy\n",
    "        \"\"\"\n",
    "        \n",
    "        statistic_manager = dict()\n",
    "        # passage level prob/entropy median\n",
    "        doc_prob_median, doc_entropy_median = MedianFinder(), MedianFinder()\n",
    "\n",
    "        for method in [\"normal\", \"weighted\"]:\n",
    "            # moving statistics in sentence level\n",
    "            if self.moving_mdoe == \"sentence\": \n",
    "                for sentence in self.sentence_state:\n",
    "                    sentence_prob, sentence_entropy = MovingAverage(self.window_len),  MovingAverage(self.window_len)\n",
    "                    for state in sentence:\n",
    "                        # compute prob, entropy w.r.t prop_tag, either pos_tag or propstr\n",
    "                        prop_tag = state.prop_tag.propstr\n",
    "                        if prop_tag not in statistic_manager.keys():\n",
    "                            statistic_manager[prop_tag] = {\"probability\": MovingAverage(self.window_len),\n",
    "                                                        \"entropy\": MovingAverage(self.window_len)}\n",
    "                        self._add_num_for_stat(state.entropy if method==\"normal\"else state.statistics.weighted_entropy, \n",
    "                                                [statistic_manager[prop_tag][\"entropy\"], \n",
    "                                                doc_entropy_median, sentence_entropy])\n",
    "                        self._add_num_for_stat(state.probability if method==\"normal\"else state.statistics.weighted_prob,\n",
    "                                            [statistic_manager[prop_tag][\"probability\"],\n",
    "                                            doc_prob_median, sentence_prob])\n",
    "                        # doc median\n",
    "                        moving_doc_median = {\"entropy\":doc_entropy_median.findMedian(), \"probability\":doc_prob_median.findMedian()}\n",
    "                        # sentence avg, rmse\n",
    "                        sentence_entropy_avg, sentence_entropy_rmse = sentence_entropy.get_cur_state()\n",
    "                        sentence_prob_avg, sentence_prob_rmse = sentence_prob.get_cur_state()\n",
    "                        # tag avg, rmse\n",
    "                        tag_entropy_avg, tag_entropy_rmse = statistic_manager[prop_tag][\"entropy\"].get_cur_state()\n",
    "                        tag_prob_avg, tag_prob_rmse = statistic_manager[prop_tag][\"probability\"].get_cur_state()\n",
    "                        \n",
    "                        if method == \"normal\":\n",
    "                            state.statistics.moving_doc_median = moving_doc_median\n",
    "                            state.statistics.moving_rmse = {\"sentence_entropy\": sentence_entropy_rmse, \"sentence_prob\": sentence_prob_rmse,\n",
    "                                        \"tag_entropy\": tag_entropy_rmse, \"tag_prob\": tag_prob_rmse}\n",
    "                            state.statistics.moving_mean = {\"sentence_entropy\": sentence_entropy_avg, \"sentence_prob\": sentence_prob_avg,\n",
    "                                        \"tag_entropy\": tag_entropy_avg, \"tag_prob\": tag_prob_avg}\n",
    "                        else:                     \n",
    "                            # weighted statistics\n",
    "                            state.statistics.weighted_moving_doc_median = moving_doc_median\n",
    "                            state.statistics.weighted_moving_rmse = {\"sentence_entropy\": sentence_entropy_rmse, \"sentence_prob\": sentence_prob_rmse,\n",
    "                                        \"tag_entropy\": tag_entropy_rmse, \"tag_prob\": tag_prob_rmse}\n",
    "                            state.statistics.weighted_moving_mean = {\"sentence_entropy\": sentence_entropy_avg, \"sentence_prob\": sentence_prob_avg,\n",
    "                                        \"tag_entropy\": tag_entropy_avg, \"tag_prob\": tag_prob_avg}\n",
    "            \n",
    "            # moving statistics in passage level\n",
    "            elif self.moving_mdoe == \"passage\":\n",
    "                    doc_prob, doc_entropy = MovingAverage(self.window_len),  MovingAverage(self.window_len)\n",
    "                    for state in self.states:\n",
    "                        # compute prob, entropy w.r.t prop_tag, either pos_tag or propstr\n",
    "                        prop_tag = state.prop_tag.propstr\n",
    "                        if prop_tag not in statistic_manager.keys():\n",
    "                            statistic_manager[prop_tag] = {\"probability\": MovingAverage(self.window_len),\n",
    "                                                        \"entropy\": MovingAverage(self.window_len)}\n",
    "                        self._add_num_for_stat(state.entropy if method==\"normal\"else state.statistics.weighted_entropy, \n",
    "                                                [statistic_manager[prop_tag][\"entropy\"], \n",
    "                                                doc_entropy_median, doc_entropy])\n",
    "                        self._add_num_for_stat(state.probability if method==\"normal\"else state.statistics.weighted_prob,\n",
    "                                            [statistic_manager[prop_tag][\"probability\"],\n",
    "                                            doc_prob_median, doc_prob])\n",
    "                        # doc median\n",
    "                        moving_doc_median = {\"entropy\":doc_entropy_median.findMedian(), \"probability\":doc_prob_median.findMedian()}\n",
    "                        # doc avg, rmse\n",
    "                        doc_entropy_avg, doc_entropy_rmse = doc_entropy.get_cur_state()\n",
    "                        doc_prob_avg, doc_prob_rmse = doc_prob.get_cur_state()\n",
    "                        # tag avg, rmse\n",
    "                        tag_entropy_avg, tag_entropy_rmse = statistic_manager[prop_tag][\"entropy\"].get_cur_state()\n",
    "                        tag_prob_avg, tag_prob_rmse = statistic_manager[prop_tag][\"probability\"].get_cur_state()\n",
    "                        \n",
    "                        if method == \"normal\":\n",
    "                            state.statistics.moving_doc_median = moving_doc_median\n",
    "                            state.statistics.moving_rmse = {\"doc_entropy\": doc_entropy_rmse, \"doc_prob\": doc_prob_rmse,\n",
    "                                        \"tag_entropy\": tag_entropy_rmse, \"tag_prob\": tag_prob_rmse}\n",
    "                            state.statistics.moving_mean = {\"doc_entropy\": doc_entropy_avg, \"doc_prob\": doc_prob_avg,\n",
    "                                        \"tag_entropy\": tag_entropy_avg, \"tag_prob\": tag_prob_avg}\n",
    "                        else:                     \n",
    "                            # weighted statistics\n",
    "                            state.statistics.weighted_moving_doc_median = moving_doc_median\n",
    "                            state.statistics.weighted_moving_rmse = {\"doc_entropy\": doc_entropy_rmse, \"doc_prob\": sentence_prob_rmse,\n",
    "                                        \"tag_entropy\": tag_entropy_rmse, \"tag_prob\": tag_prob_rmse}\n",
    "                            state.statistics.weighted_moving_mean = {\"doc_entropy\": doc_entropy_avg, \"doc_prob\": sentence_prob_avg,\n",
    "                                        \"tag_entropy\": tag_entropy_avg, \"tag_prob\": tag_prob_avg}\n",
    "\n",
    "            else:\n",
    "                print(\"Mode not found! Either sentence or passage. \\n\")\n",
    "\n",
    "class MedianFinder:\n",
    "    # Refer to: https://github.com/criszhou/LeetCode-Python/blob/master/295.%20Find%20Median%20from%20Data%20Stream.py\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize your data structure here.\n",
    "        \"\"\"\n",
    "        self.upperHeap = [float('inf')]\n",
    "        self.lowerHeap = [float('inf')]\n",
    "        # lowerHeap's numbers are minus original numbers, because in Python heap is min-heap\n",
    "\n",
    "        # always maintain that their lens are equal, or upper has 1 more than lower\n",
    "\n",
    "    def add_num(self, num):\n",
    "        \"\"\"\n",
    "        Adds a num into the data structure.\n",
    "        :type num: int\n",
    "        :rtype: void\n",
    "        \"\"\"\n",
    "        upperMin = + self.upperHeap[0]\n",
    "        lowerMax = - self.lowerHeap[0]\n",
    "\n",
    "        if num > upperMin or (lowerMax<=num<=upperMin and len(self.upperHeap)==len(self.lowerHeap)):\n",
    "            heappush(self.upperHeap, num)\n",
    "        else:\n",
    "            heappush(self.lowerHeap, -num)\n",
    "\n",
    "        # maintain the invariant that their lens are equal, or upper has 1 more than lower\n",
    "        if len(self.upperHeap)-len(self.lowerHeap) > 1:\n",
    "            heappush( self.lowerHeap, -heappop( self.upperHeap ) )\n",
    "        elif len(self.lowerHeap) > len(self.upperHeap):\n",
    "            heappush( self.upperHeap, -heappop( self.lowerHeap ) )\n",
    "\n",
    "\n",
    "    def findMedian(self):\n",
    "        \"\"\"\n",
    "        Returns the median of current data stream\n",
    "        :rtype: float\n",
    "        \"\"\"\n",
    "        if len(self.upperHeap) == len(self.lowerHeap):\n",
    "            upperMin = + self.upperHeap[0]\n",
    "            lowerMax = - self.lowerHeap[0]\n",
    "            return ( float(upperMin) + float(lowerMax) ) / 2.0\n",
    "        else:\n",
    "            assert len(self.upperHeap) == len(self.lowerHeap) + 1\n",
    "            return float(self.upperHeap[0])\n",
    "    \n",
    "class MovingAverage:\n",
    "    def __init__(self, window_size):\n",
    "        \"\"\"\n",
    "        Compute moving average for given data array and window_size.\n",
    "            When len(data) < window_size, compute moving average without window_size constraint\n",
    "            When len(data) > window_size, the fisrt window_size - 1 components is computed by\n",
    "                moving average without constraint also.\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.values = np.zeros(window_size)\n",
    "        self.sum = 0\n",
    "        self.index = 0\n",
    "        self.count = 0\n",
    "        self.recorder = list()\n",
    "\n",
    "    def add_num(self, num):\n",
    "        if self.count < self.window_size:\n",
    "            self.count += 1\n",
    "        else:\n",
    "            self.sum = self.sum - self.values[self.index]\n",
    "        self.sum += num\n",
    "        self.values[self.index] = num\n",
    "        self.index = (self.index + 1) % self.window_size\n",
    "        self.recorder.append(num)\n",
    "\n",
    "    def get_average(self):\n",
    "        if self.count == 0:\n",
    "            return None\n",
    "        return self.sum / self.count\n",
    "    \n",
    "    def get_cur_state(self):\n",
    "        avg =  self.get_average()\n",
    "        errors = np.array(self.recorder[-self.window_size:]) - avg\n",
    "        squared_errors = errors ** 2\n",
    "        mse = np.mean(squared_errors)\n",
    "        return avg, np.sqrt(mse)\n",
    "    \n",
    "    def get_rmse(self):\n",
    "        avg, rmse = self.get_cur_state()\n",
    "        return rmse\n",
    "\n",
    "\n",
    "def get_spacy_info(doc:spacy.tokens.doc.Doc):\n",
    "    \"\"\"\n",
    "    Get information from spacy pipeline for every token\n",
    "    \"\"\"\n",
    "    result = OrderedDict()\n",
    "    left_idx = 0\n",
    "\n",
    "    for i, token in enumerate(doc):\n",
    "        right_idx = left_idx + len(token)\n",
    "        result[(left_idx, right_idx)] = dict()\n",
    "        # assign token attributes\n",
    "        for key in SPACY_INFO_DICT:\n",
    "            result[(left_idx, right_idx)][key] = getattr(token, key)\n",
    "        # token’s immediate syntactic children\n",
    "        result[(left_idx, right_idx)][\"children\"] = [child for child in token.children]\n",
    "        # if is new sentnce\n",
    "        if i > 0 and token.tag_ == \".\":\n",
    "            result[(left_idx, right_idx)][\"new_sentence\"] = True\n",
    "        else:\n",
    "            result[(left_idx, right_idx)][\"new_sentence\"] = False\n",
    "        left_idx = right_idx\n",
    "    return result\n",
    "\n",
    "def get_llm_info(text:str, llm_token:list, top_k_prob:np.ndarray, top_k_token:np.ndarray):\n",
    "    \"\"\"\n",
    "    Get information from LLM: token, top-k prob, top-k alternative token, entropy\n",
    "    \"\"\"\n",
    "    result = OrderedDict()\n",
    "    token_left_idx = 0\n",
    "    text_left_inx = 0\n",
    "\n",
    "    for i, token in enumerate(llm_token):\n",
    "        token_right_idx = token_left_idx + len(token)\n",
    "        word = text[token_left_idx: token_right_idx]\n",
    "        \n",
    "        # check token-word alignment\n",
    "        if word != token:\n",
    "            print(f\"word-token mismatch: {word} - {token}\")\n",
    "            break\n",
    "        # remove beginning space\n",
    "        if token.startswith(\" \"):\n",
    "            word = word[1:]\n",
    "        # compute Shannon entropy\n",
    "        token_entropy = uncertainty(top_k_prob[i,:])[1]\n",
    "        # assign attributes to token \n",
    "        text_right_idx = text_left_inx + len(word)\n",
    "        result[(text_left_inx, text_right_idx)] = {'text': word, \n",
    "                                                   \"llm_token_idx\": i,\n",
    "                                                   \"top_k_prob\": top_k_prob[i,:].tolist(),\n",
    "                                                   \"top_k_token\": top_k_token[i,:].tolist(),\n",
    "                                                   \"entropy\": token_entropy}\n",
    "        text_left_inx, token_left_idx = text_right_idx, token_right_idx\n",
    "    return result\n",
    "\n",
    "def align_llm_spacy_output(nlp_result:OrderedDict, llm_result:OrderedDict, verbose = False):\n",
    "    \"\"\"\n",
    "    Aligh the result return by `get_spacy_info` and `get_llm_info`\n",
    "    Return: merged token info -> semantics info + statistical info\n",
    "    \"\"\"\n",
    "    result = OrderedDict()\n",
    "    nlp_idx, llm_idx = np.array(list(nlp_result.keys())), np.array(list(llm_result.keys()))\n",
    "    llm_start_idx, llm_end_idx = llm_idx[:,0], llm_idx[:,1]\n",
    "    # word not found in llm, word not match in llm\n",
    "    num_word_not_found, num_word_not_match = 0, 0\n",
    "\n",
    "    for idx_range in nlp_result:\n",
    "        left_idx, right_idx = idx_range\n",
    "\n",
    "        # check idx alignmnet\n",
    "        # neither starting/ending idx exist in llm_idx list\n",
    "        if (left_idx not in llm_idx) or (right_idx not in llm_idx):\n",
    "            # if verbose: print(f\"word: {nlp_result(idx_range)} not found in llm result\")\n",
    "            result[idx_range] = {\"text\": nlp_result[idx_range][\"text\"],\n",
    "                                \"nlp_result\": nlp_result[idx_range],\n",
    "                                \"llm_result\": None,\n",
    "                                 \"is_found\": False, # not found at all\n",
    "                                 \"is_match\": False}  # found but idx mismatch\n",
    "            num_word_not_found += 1\n",
    "            continue\n",
    "        # idx mismatch: word in nlp contains multiple token in llm results\n",
    "        elif idx_range not in llm_result:\n",
    "            cross_llm_idx = [list(llm_start_idx).index(left_idx), list(llm_end_idx).index(right_idx)]\n",
    "            \n",
    "            result[idx_range] = {\"text\": nlp_result[idx_range][\"text\"],\n",
    "                                \"nlp_result\": nlp_result[idx_range],\n",
    "                                \"llm_result\": [llm_result[tuple(llm_idx[i])] for i in cross_llm_idx],\n",
    "                                \"is_found\": True,\n",
    "                                \"is_match\": False}\n",
    "            num_word_not_match += 1\n",
    "        # exact match\n",
    "        else:\n",
    "            result[idx_range] = {\"text\": nlp_result[idx_range][\"text\"],\n",
    "                                \"nlp_result\": nlp_result[idx_range],\n",
    "                                \"llm_result\": [llm_result[idx_range]],\n",
    "                                \"is_found\": True,\n",
    "                                \"is_match\": True}\n",
    "    if verbose: print(f\"num of word not found: {num_word_not_found} \\n\" + \n",
    "                      f\"num of word not matched: {num_word_not_match} \\n\")\n",
    "    return result\n",
    "\n",
    "def get_model_state(align_output:OrderedDict): \n",
    "    \"\"\"\n",
    "    Parse aligned spacy/llm output and convert to state format\n",
    "    Output: a list of `State` classes\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for i, token in enumerate(list(align_output.items())):\n",
    "        token_info = token[1]\n",
    "        # if multiple llm result available, take the first one for state entropy and prob \n",
    "        word_state = State(i,\n",
    "                           token_info[\"text\"], \n",
    "                           token_info[\"llm_result\"][0][\"entropy\"],  \n",
    "                           token_info[\"llm_result\"][0][\"top_k_prob\"][0],\n",
    "                           token_info[\"llm_result\"],\n",
    "                           token_info[\"nlp_result\"],\n",
    "                           token_info,\n",
    "                           statistics=StateStatics())\n",
    "        # try:\n",
    "        word_state.prop_tag = PropTag(word_state.spacy_info[\"pos_\"])\n",
    "        # except:\n",
    "        #     print(word_state.spacy_info[\"pos_\"])\n",
    "        #     break\n",
    "        result.append(word_state)\n",
    "    result = get_children(result, NOUN_CHILDREN, VERB_CHILDREN)\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_children(states:State, NOUN_CHILDREN:set, VERB_CHILDREN:set):\n",
    "    \"\"\"\n",
    "    Assign children token/state info to each state\n",
    "    \"\"\"\n",
    "    for state in states:\n",
    "        result = dict()\n",
    "        children = state.spacy_info[\"children\"]\n",
    "        # attach children for NOUN state\n",
    "        if len(children) > 0 and state.prop_tag.propstr==\"NOUN\":\n",
    "            for child in children:\n",
    "                child = states[child.i] # locate child crossponding state\n",
    "                if child.prop_tag.pos_tag in NOUN_CHILDREN:\n",
    "                    result[child.token] = child.idx     #[child_text: child_idx_in_states]\n",
    "            state.children = result\n",
    "            continue\n",
    "        # attach children for VERB state\n",
    "        elif len(children) > 0 and state.prop_tag.propstr==\"VERB\":\n",
    "            for child in children:\n",
    "                child = states[child.i] # locate child crossponding state\n",
    "                if child.prop_tag.pos_tag in VERB_CHILDREN:\n",
    "                    result[child.token] = child.idx     #[child_text: child_idx_in_states]\n",
    "            state.children = result\n",
    "        else:\n",
    "            state.children = result\n",
    "    return states\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset name: reponse_eli5_category_train_100.json \n",
      "num of questions: 100 \n",
      "dataset keys: ['question', 'answer', 'prompt', 'text', 'token', 'top_k_token', 'top_k_prob', 'top_logprobs', 'prompt_tokens', 'completion_tokens', 'response'] \n",
      "\n",
      "\n",
      "Question: Why is it that we calm down when we take a deep breath, hold it for a few seconds and exhale? \n",
      "\n",
      "Response: '\\n\\nTaking a deep breath helps to activate the parasympathetic nervous system, which is responsible for calming the body down. When we take a deep breath, we are sending a signal to the brain to relax and slow down. Holding the breath for a few seconds helps to increase the oxygen levels in the body, which can help to reduce stress and anxiety. Exhaling helps to release the tension in the body and can help to reduce stress levels.' \n",
      "\n",
      "Answer: 'Anxiety/stress are the result of your sympathetic nervous system being activated (fight or flight response). When we are threatened we experience a variety of physical effects, e.g. increased heart rate, GI upset, jittery muscles, *rapid shallow breathing*. This helps us fight the threat or run away from it. Helpful when facing down a sabre-toothed tiger; not so much when we can\\'t speak to a large group. The parasympathetic nervous system works in opposition to the sympathetic nervous system. It\\'s the \"all is well\" setting for your bod- relaxation in terms of muscle and heart rate, *slow deep abdominal breathing*. What intentional deep breathing does is activate the parasympathetic nervous system, even if there is an anxiety-provoking event. Basically you are overriding the stress response with the \"nothing to worry about\" system. Since you can\\'t simultaneously be threatened and unthreatened, your body takes the cue from the slow breathing to reduce or eliminate entirely the stress response.' \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load sample response from datasets\n",
    "load_path = \"/Users/jiayangsong/Documents/git/LLM_analysis/hallucination_detection/response_data/\"\n",
    "dataset_name = \"reponse_eli5_category_train_100.json\"\n",
    "\n",
    "df = pd.read_json(load_path+dataset_name)\n",
    "\n",
    "print(f\"dataset name: {dataset_name} \\n\" + \n",
    "      f\"num of questions: {len(df)} \\n\" +\n",
    "      f\"dataset keys: {list(df.columns)} \\n\\n\")\n",
    "\n",
    "# take first reponse for later processing \n",
    "sample_response = df.iloc[3]\n",
    "\n",
    "# extract response info \n",
    "num_token_response = sample_response[\"completion_tokens\"]\n",
    "top_k_prob = np.array(sample_response[\"top_k_prob\"])\n",
    "top_k_token = np.array(sample_response[\"top_k_token\"])\n",
    "top_logprobs = sample_response[\"top_logprobs\"]\n",
    "question = sample_response[\"question\"]\n",
    "text = sample_response[\"text\"]\n",
    "llm_token = sample_response[\"token\"]\n",
    "\n",
    "# display question, response from llm,  true answer\n",
    "print(f\"Question: {sample_response['question']} \\n\\n\" + \n",
    "      f\"Response: {repr(''.join((sample_response['text'])))} \\n\\n\" + \n",
    "      f\"Answer: {repr(''.join((sample_response['answer'])))} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "doc = nlp(text)\n",
    "nlp_result = get_spacy_info(doc)\n",
    "llm_result = get_llm_info(text, llm_token, top_k_prob, top_k_token)\n",
    "align_result = align_llm_spacy_output(nlp_result, llm_result, verbose=False)\n",
    "states = get_model_state(align_result)\n",
    "final_states = StateOverview(states, 4, (\"NOUN\", \"VERB\"), 0.5, \"sentence\", nlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
